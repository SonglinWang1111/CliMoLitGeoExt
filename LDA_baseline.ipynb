{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6d79e6-1437-4380-b818-f29c8028c615",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d96b6b-ae31-458e-81fc-70664343ae01",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation, or LDA, is a 3-level hierarchical Bayesian model. Put it differently, it is a generative statistic model that explains how a collection of text documents can be described by a set of unobserved topics. Each item of the collection is modelled as a finite mixture over a latent set of topics here, while each topic is characterized by a distribution of words. As a generalization of pLSA model, it differs primarily by treating the topic mixture as a Dirichlet prior, leading to more reasonable mixtures and less susceptibility to overfitting. LDA is an important model in NLP, solving the problem of topic discovery, similarity comparison, document modeling and classification.\n",
    "\n",
    "As for more detail of this model, basically, it is a Bag-of-Word based model, based on word co-occurrance. It considers that a piece of text is composed of many words, without considering order. One piece of text can have many topics, while each word from it is generated by one of the topics. The first step of this model is, to get document i a topic distribution theta(i) from Dirichlet distribution alpha. Then, from theta(i), it gets topic number j for document i as z(i,j). Next, from Dirichlet distribution beta, it generates the word distribution of z(i,j) then sample the final word w(i,j). With Gibbs sampling and EM model, we can convergent to the final result. In summary, alpha and beta are corpus level parameters, and z and w are word level variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc8d25-157b-4c2d-9539-47e475ffd631",
   "metadata": {},
   "source": [
    "![LDA](Latent_Dirichlet_allocation.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27cd0a-3a02-4031-84a0-0f774445aebc",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ea64f4-a5e9-45d8-b8cf-c4c4b82df245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.parsing.preprocessing import preprocess_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad654c65-729a-4d48-91f8-575deb8cb9a0",
   "metadata": {},
   "source": [
    "### Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c42213-315b-4db4-8ee0-577fae53789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('literature.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275053c9-d3cb-4ce0-bde1-678bc5342d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract papers from dictionary and save in a list\n",
    "texts = []\n",
    "\n",
    "for _, sections in data.items():\n",
    "    full_text = \" \".join(sections.values())\n",
    "    texts.append(full_text)\n",
    "\n",
    "# function from gensim, can delete stop words, transfer to lower case, etc.\n",
    "processed_texts = [preprocess_string(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ba0085-1513-4f43-bfa2-e0e187af69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(processed_texts) # construct dictionary from the papers\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts] # construct corpus, each paper is transferred into a list of (word_id, word_count) tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d50226-6fc2-4985-ae58-e054a1b07733",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc9fe3-9821-493f-a461-c0ac91d1031a",
   "metadata": {},
   "source": [
    "Usually we have to train model with our own data while using LDA, especially for minor disciplines. Otherwise the topics can not match well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa9b381-1bf2-4ee5-b64b-5ffbac8d2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=50, passes=50, random_state=42) # LdaMulticore (..., workers=8, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73540514-6a75-46df-a248-53b219f734e6",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a783e-73c8-4444-ae4c-3d90da64b82b",
   "metadata": {},
   "source": [
    "lda_model.show_topics(num_topics=10, num_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a44c6fb-cc30-47db-9c6f-d5494bf274b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(33, np.float32(0.96535563)), (42, np.float32(0.01416937))]\n",
      "[(7, np.float32(0.045853946)), (19, np.float32(0.74599874)), (31, np.float32(0.020226821)), (40, np.float32(0.16536564))]\n",
      "[(31, np.float32(0.0266248)), (42, np.float32(0.0571256)), (49, np.float32(0.91036904))]\n",
      "[(31, np.float32(0.9819842))]\n",
      "[(1, np.float32(0.59377176)), (4, np.float32(0.10625016)), (7, np.float32(0.027269533)), (21, np.float32(0.13394631)), (25, np.float32(0.0102562765)), (29, np.float32(0.08460806)), (35, np.float32(0.04128924))]\n",
      "[(31, np.float32(0.9997854))]\n",
      "[(1, np.float32(0.047283813)), (4, np.float32(0.88994294)), (19, np.float32(0.04369402)), (35, np.float32(0.018097706))]\n",
      "[(8, np.float32(0.5319542)), (25, np.float32(0.14291236)), (31, np.float32(0.061975256)), (35, np.float32(0.2308555))]\n",
      "[(8, np.float32(0.9853141))]\n",
      "[(25, np.float32(0.964285)), (42, np.float32(0.03551389))]\n",
      "[(7, np.float32(0.99975896))]\n",
      "[(25, np.float32(0.01275665)), (31, np.float32(0.011655295)), (40, np.float32(0.96094334))]\n",
      "[(42, np.float32(0.9997182))]\n",
      "[(14, np.float32(0.031595506)), (23, np.float32(0.9606252))]\n",
      "[(7, np.float32(0.036607184)), (8, np.float32(0.015192539)), (25, np.float32(0.1288795)), (29, np.float32(0.10119009)), (35, np.float32(0.7178188))]\n",
      "[(7, np.float32(0.034433927)), (8, np.float32(0.027881566)), (25, np.float32(0.06792533)), (29, np.float32(0.86055326))]\n",
      "[(25, np.float32(0.69876236)), (29, np.float32(0.25262102)), (35, np.float32(0.048356))]\n",
      "[(5, np.float32(0.8812513)), (7, np.float32(0.030801352)), (31, np.float32(0.012886474)), (35, np.float32(0.070722))]\n",
      "[(4, np.float32(0.031038107)), (33, np.float32(0.9674647))]\n",
      "[(4, np.float32(0.20053437)), (7, np.float32(0.34173858)), (8, np.float32(0.4473167))]\n",
      "[(5, np.float32(0.41609067)), (7, np.float32(0.5835675))]\n",
      "[(8, np.float32(0.13921773)), (29, np.float32(0.55831045)), (31, np.float32(0.08739065)), (40, np.float32(0.022781655)), (42, np.float32(0.039309498)), (49, np.float32(0.13848327))]\n",
      "[(1, np.float32(0.9997378))]\n",
      "[(35, np.float32(0.99868596))]\n",
      "[(7, np.float32(0.5003002)), (8, np.float32(0.1348065)), (21, np.float32(0.03251954)), (31, np.float32(0.016446382)), (33, np.float32(0.22224218)), (35, np.float32(0.08568184))]\n",
      "[(8, np.float32(0.19028948)), (29, np.float32(0.05544742)), (31, np.float32(0.031159747)), (35, np.float32(0.7083382)), (49, np.float32(0.014517093))]\n",
      "[(1, np.float32(0.022117004)), (8, np.float32(0.14702335)), (14, np.float32(0.70614266)), (31, np.float32(0.08836378)), (40, np.float32(0.027154343))]\n",
      "[(7, np.float32(0.016412655)), (21, np.float32(0.9431209)), (31, np.float32(0.03661131))]\n",
      "[(8, np.float32(0.46432996)), (25, np.float32(0.025584368)), (33, np.float32(0.22971399)), (35, np.float32(0.2768723))]\n",
      "[(19, np.float32(0.045412835)), (21, np.float32(0.91265625)), (49, np.float32(0.041570656))]\n"
     ]
    }
   ],
   "source": [
    "for _, bow in enumerate(corpus):\n",
    "    print(lda_model.get_document_topics(bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513b4f7-f33c-4df4-9c53-a6b6534f4252",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "\n",
    "for _, bow in enumerate(corpus):\n",
    "    dist = lda_model.get_document_topics(bow, minimum_probability=0)\n",
    "    topic = np.array([prob for _, prob in dist])\n",
    "    topics.append(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a8e1c5-dc6b-47a3-b080-58070010d20e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b70c84-f4d3-4484-94a4-b54ab86cbe1f",
   "metadata": {},
   "source": [
    "LDA can automize text classification part, however, it can not really assign text with externally defined codes for top-down paper review, especially when pre-defined class names are not present in the training data and the dictionaty it generates. Thus, usually we have to manually annotate the papers based on topics we get, top words of such topics, and the matching topics of each paper. But anyway, LDA can lower the workload compared with doing fully manual annotation. Here we first take paper 0 for evaluation. Its topic is likely to be 33 and 42 according to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe7648a6-1a4b-445b-b4fc-a0cb59819bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040*\"migrat\" + 0.024*\"environment\" + 0.023*\"household\" + 0.016*\"event\" + 0.013*\"individu\" + 0.010*\"climat\" + 0.010*\"migrant\" + 0.010*\"commun\" + 0.009*\"stressor\" + 0.009*\"chang\" + 0.009*\"intent\" + 0.008*\"zone\" + 0.008*\"relat\" + 0.008*\"head\" + 0.008*\"studi\" + 0.007*\"peopl\" + 0.007*\"transit\" + 0.007*\"like\" + 0.007*\"level\" + 0.006*\"respond\" + 0.006*\"affect\" + 0.006*\"term\" + 0.006*\"ghana\" + 0.006*\"econom\" + 0.006*\"decis\" + 0.006*\"variabl\" + 0.005*\"educ\" + 0.005*\"countri\" + 0.005*\"differ\" + 0.005*\"result\" + 0.005*\"factor\" + 0.005*\"model\" + 0.004*\"land\" + 0.004*\"adapt\" + 0.004*\"non\" + 0.004*\"impact\" + 0.004*\"member\" + 0.004*\"forest\" + 0.004*\"percept\" + 0.004*\"major\" + 0.004*\"savannah\" + 0.004*\"includ\" + 0.003*\"area\" + 0.003*\"effect\" + 0.003*\"associ\" + 0.003*\"long\" + 0.003*\"rural\" + 0.003*\"intern\" + 0.003*\"year\" + 0.003*\"black\" + 0.003*\"ag\" + 0.003*\"crop\" + 0.003*\"agricultur\" + 0.003*\"influenc\" + 0.003*\"survei\" + 0.003*\"sudden\" + 0.003*\"stai\" + 0.003*\"control\" + 0.003*\"develop\" + 0.003*\"locat\" + 0.003*\"hunter\" + 0.003*\"region\" + 0.003*\"increas\" + 0.003*\"incom\" + 0.003*\"indic\" + 0.003*\"rainfal\" + 0.003*\"examin\" + 0.003*\"provid\" + 0.003*\"type\" + 0.003*\"farmer\" + 0.003*\"higher\" + 0.003*\"strategi\" + 0.003*\"social\" + 0.002*\"research\" + 0.002*\"popul\" + 0.002*\"demograph\" + 0.002*\"data\" + 0.002*\"drought\" + 0.002*\"sever\" + 0.002*\"flood\" + 0.002*\"vulner\" + 0.002*\"mention\" + 0.002*\"reason\" + 0.002*\"resourc\" + 0.002*\"tabl\" + 0.002*\"current\" + 0.002*\"short\" + 0.002*\"livelihood\" + 0.002*\"onset\" + 0.002*\"bush\" + 0.002*\"time\" + 0.002*\"depend\" + 0.002*\"experienc\" + 0.002*\"compar\" + 0.002*\"gradual\" + 0.002*\"score\" + 0.002*\"relationship\" + 0.002*\"sampl\" + 0.002*\"respons\" + 0.002*\"statu\" + 0.002*\"likelihood\" + 0.002*\"socio\" + 0.002*\"addit\" + 0.002*\"signific\" + 0.002*\"import\" + 0.002*\"half\" + 0.002*\"averag\" + 0.002*\"consid\" + 0.002*\"adger\" + 0.002*\"perceiv\" + 0.002*\"instanc\" + 0.002*\"urban\" + 0.002*\"lower\" + 0.002*\"specif\" + 0.002*\"environ\" + 0.002*\"van\" + 0.002*\"base\" + 0.002*\"difficult\" + 0.002*\"find\" + 0.002*\"gss\" + 0.002*\"suggest\" + 0.002*\"challeng\" + 0.002*\"recent\" + 0.002*\"determin\" + 0.002*\"high\" + 0.002*\"characterist\" + 0.002*\"polit\" + 0.002*\"set\" + 0.002*\"empir\" + 0.002*\"food\" + 0.002*\"gener\" + 0.002*\"place\" + 0.002*\"abl\" + 0.002*\"issu\" + 0.002*\"report\" + 0.002*\"address\" + 0.002*\"expect\" + 0.002*\"probabl\" + 0.001*\"stress\" + 0.001*\"easili\" + 0.001*\"follow\" + 0.001*\"measur\" + 0.001*\"risk\" + 0.001*\"lead\" + 0.001*\"us\" + 0.001*\"size\" + 0.001*\"live\" + 0.001*\"opportun\" + 0.001*\"case\" + 0.001*\"statist\" + 0.001*\"low\" + 0.001*\"number\" + 0.001*\"support\" + 0.001*\"polici\" + 0.001*\"exampl\" + 0.001*\"marri\" + 0.001*\"new\" + 0.001*\"irregular\" + 0.001*\"hand\" + 0.001*\"contribut\" + 0.001*\"poor\" + 0.001*\"focu\" + 0.001*\"condit\" + 0.001*\"actual\" + 0.001*\"identifi\" + 0.001*\"final\" + 0.001*\"cost\" + 0.001*\"lack\" + 0.001*\"tend\" + 0.001*\"ask\" + 0.001*\"concern\" + 0.001*\"understand\" + 0.001*\"natur\" + 0.001*\"product\" + 0.001*\"assess\" + 0.001*\"experi\" + 0.001*\"significantli\" + 0.001*\"reli\" + 0.001*\"person\" + 0.001*\"slow\" + 0.001*\"usual\" + 0.001*\"order\" + 0.001*\"regard\" + 0.001*\"indigen\" + 0.001*\"share\" + 0.001*\"second\" + 0.001*\"storm\" + 0.001*\"mcleman\" + 0.001*\"africa\" + 0.001*\"inform\" + 0.001*\"decid\" + 0.001*\"farm\" + 0.001*\"potenti\" + 0.001*\"futur\" + 0.001*\"problem\" + 0.001*\"role\" + 0.001*\"main\" + 0.001*\"der\" + 0.001*\"process\" + 0.001*\"show\"\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.print_topic(33, topn=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "551a7d6e-7172-4a1b-8a11-a53995ea0d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025*\"hawaw\" + 0.016*\"peopl\" + 0.015*\"return\" + 0.012*\"project\" + 0.011*\"livelihood\" + 0.010*\"group\" + 0.010*\"migrat\" + 0.010*\"area\" + 0.009*\"local\" + 0.008*\"jawasir\" + 0.008*\"forc\" + 0.008*\"right\" + 0.007*\"tradit\" + 0.007*\"land\" + 0.007*\"opportun\" + 0.007*\"new\" + 0.007*\"agricultur\" + 0.006*\"irrig\" + 0.006*\"anim\" + 0.006*\"drought\" + 0.006*\"nile\" + 0.006*\"import\" + 0.005*\"displac\" + 0.005*\"sudan\" + 0.005*\"nomad\" + 0.005*\"famili\" + 0.005*\"differ\" + 0.005*\"pastoralist\" + 0.005*\"cultiv\" + 0.004*\"northern\" + 0.004*\"establish\" + 0.004*\"farm\" + 0.004*\"women\" + 0.004*\"stai\" + 0.004*\"success\" + 0.004*\"refuge\" + 0.004*\"work\" + 0.004*\"situat\" + 0.004*\"wadi\" + 0.004*\"live\" + 0.004*\"interview\" + 0.004*\"sub\" + 0.004*\"homeland\" + 0.004*\"leader\" + 0.004*\"develop\" + 0.004*\"labour\" + 0.004*\"institut\" + 0.004*\"time\" + 0.004*\"possibl\" + 0.003*\"addit\" + 0.003*\"process\" + 0.003*\"environment\" + 0.003*\"surviv\" + 0.003*\"rain\" + 0.003*\"provid\" + 0.003*\"secur\" + 0.003*\"number\" + 0.003*\"rainfal\" + 0.003*\"famin\" + 0.003*\"social\" + 0.003*\"ethnic\" + 0.003*\"sustain\" + 0.003*\"construct\" + 0.003*\"includ\" + 0.003*\"belong\" + 0.003*\"muggadam\" + 0.003*\"adra\" + 0.003*\"year\" + 0.003*\"resourc\" + 0.003*\"farmer\" + 0.003*\"intern\" + 0.003*\"state\" + 0.003*\"govern\" + 0.003*\"plot\" + 0.003*\"term\" + 0.003*\"reason\" + 0.003*\"place\" + 0.003*\"creat\" + 0.003*\"hunger\" + 0.003*\"base\" + 0.003*\"problem\" + 0.003*\"poor\" + 0.002*\"product\" + 0.002*\"migrant\" + 0.002*\"good\" + 0.002*\"returne\" + 0.002*\"failur\" + 0.002*\"appli\" + 0.002*\"fed\" + 0.002*\"recognis\" + 0.002*\"studi\" + 0.002*\"water\" + 0.002*\"select\" + 0.002*\"sorghum\" + 0.002*\"fund\" + 0.002*\"servic\" + 0.002*\"committe\" + 0.002*\"individu\" + 0.002*\"natur\" + 0.002*\"men\" + 0.002*\"abl\" + 0.002*\"feel\" + 0.002*\"perceiv\" + 0.002*\"chang\" + 0.002*\"visit\" + 0.002*\"human\" + 0.002*\"research\" + 0.002*\"access\" + 0.002*\"relat\" + 0.002*\"given\" + 0.002*\"season\" + 0.002*\"difficult\" + 0.002*\"result\" + 0.002*\"wai\" + 0.002*\"follow\" + 0.002*\"usual\" + 0.002*\"crop\" + 0.002*\"market\" + 0.002*\"flood\" + 0.002*\"north\" + 0.002*\"disast\" + 0.002*\"mid\" + 0.002*\"strategi\" + 0.002*\"need\" + 0.002*\"cope\" + 0.002*\"avail\" + 0.002*\"member\" + 0.002*\"affect\" + 0.002*\"inform\" + 0.002*\"trade\" + 0.002*\"went\" + 0.002*\"report\" + 0.002*\"factor\" + 0.002*\"distribut\" + 0.002*\"staff\" + 0.002*\"activ\" + 0.002*\"undertaken\" + 0.002*\"physic\" + 0.002*\"purpos\" + 0.002*\"mobil\" + 0.002*\"fertil\" + 0.002*\"relief\" + 0.002*\"ident\" + 0.002*\"choos\" + 0.001*\"settl\" + 0.001*\"old\" + 0.001*\"school\" + 0.001*\"defin\" + 0.001*\"region\" + 0.001*\"stress\" + 0.001*\"repatri\" + 0.001*\"camel\" + 0.001*\"omdurman\" + 0.001*\"fodder\" + 0.001*\"user\" + 0.001*\"period\" + 0.001*\"exampl\" + 0.001*\"increas\" + 0.001*\"livestock\" + 0.001*\"level\" + 0.001*\"trigger\" + 0.001*\"better\" + 0.001*\"choic\" + 0.001*\"todai\" + 0.001*\"reconstruct\" + 0.001*\"network\" + 0.001*\"capit\" + 0.001*\"option\" + 0.001*\"movement\" + 0.001*\"goat\" + 0.001*\"health\" + 0.001*\"plai\" + 0.001*\"facilit\" + 0.001*\"experienc\" + 0.001*\"sale\" + 0.001*\"rich\" + 0.001*\"middl\" + 0.001*\"connect\" + 0.001*\"avoid\" + 0.001*\"desert\" + 0.001*\"outsid\" + 0.001*\"integr\" + 0.001*\"servant\" + 0.001*\"dig\" + 0.001*\"moh\" + 0.001*\"well\" + 0.001*\"norad\" + 0.001*\"casual\" + 0.001*\"claim\" + 0.001*\"kordofan\" + 0.001*\"exist\" + 0.001*\"site\" + 0.001*\"necessarili\" + 0.001*\"plan\" + 0.001*\"polit\" + 0.001*\"go\" + 0.001*\"contribut\" + 0.001*\"sourc\" + 0.001*\"home\" + 0.001*\"involv\"\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.print_topic(42, topn=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "884bf44b-e1cf-4e88-88d6-dea8daf6fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 1, 1, 1, \\\n",
    "        1, 1, 1, 0, 0, \\\n",
    "        1, 1, 1, 1, \\\n",
    "        1, 0, 1, 1, \\\n",
    "        1, 1, 1, \\\n",
    "        0, 1, 1, 0, 0, 0, \\\n",
    "        0, 0, 1, 1, 1, 1, 0, \\\n",
    "        0, 1, 0]]\n",
    "result = pd.DataFrame(data, columns=['Qualitative method', 'Quantitative method', 'Socio-demo-economic data', 'Environmental data', \\\n",
    "                       'Individuals', 'Households', 'Subnational groups', 'National groups', 'International groups', \\\n",
    "                       'Urban', 'Rural', 'Time frame considered', 'Foresight', \\\n",
    "                       'Rainfall pattern / Variability', 'Temperature change', 'Food scarcity / Famine / Food security', 'Drought / Aridity / Desertification', \\\n",
    "                       'Floods', 'Erosion / Soil fertility / Land degradation / Deforestation / Salinisation', 'Self assessment / Perceived environment', \\\n",
    "                       'Labour migration', 'Marriage migration', 'Refugees', 'International migration', 'Cross-border migration', 'Internal migration', \\\n",
    "                       'Rural to urban', 'Rural to rural', 'Circular / Seasonal', 'Long distance', 'Short distance', 'Temporal', 'Permanent', \\\n",
    "                       'Age', 'Gender', 'Ethnicity / Religion']).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfd92dcd-3304-439d-bbb9-ca0c0a582e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_result = pd.read_excel('manual.xlsx').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8702118-7184-4bc9-8c47-45a3f7dcc101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Qualitative method</th>\n",
       "      <th>Quantitative method</th>\n",
       "      <th>Socio-demo-economic data</th>\n",
       "      <th>Environmental data</th>\n",
       "      <th>Individuals</th>\n",
       "      <th>Households</th>\n",
       "      <th>Subnational groups</th>\n",
       "      <th>National groups</th>\n",
       "      <th>International groups</th>\n",
       "      <th>Urban</th>\n",
       "      <th>...</th>\n",
       "      <th>Rural to urban</th>\n",
       "      <th>Rural to rural</th>\n",
       "      <th>Circular / Seasonal</th>\n",
       "      <th>Long distance</th>\n",
       "      <th>Short distance</th>\n",
       "      <th>Temporal</th>\n",
       "      <th>Permanent</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnicity / Religion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Qualitative method Quantitative method Socio-demo-economic data  \\\n",
       "0                  0                   1                        1   \n",
       "\n",
       "  Environmental data Individuals Households Subnational groups  \\\n",
       "0                  1           0          1                  0   \n",
       "\n",
       "  National groups International groups Urban  ... Rural to urban  \\\n",
       "0               0                    0     0  ...              0   \n",
       "\n",
       "  Rural to rural Circular / Seasonal Long distance Short distance Temporal  \\\n",
       "0              0                   0             0              0        0   \n",
       "\n",
       "  Permanent Age Gender Ethnicity / Religion  \n",
       "0         0   1      1                    0  \n",
       "\n",
       "[1 rows x 36 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_result = manual_result.iloc[[3]].drop(columns=['ID', 'AUTHOR', 'TITLE']).reset_index(drop=True)\n",
    "manual_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d238181d-6efc-49f6-a73b-d2ab7541ba27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5277777777777778)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_result = (result.iloc[0] == manual_result.iloc[0])\n",
    "bool_result.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3c006-c31d-45d7-acb2-0e7336ba024c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myGeoKG",
   "language": "python",
   "name": "mygeokg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
